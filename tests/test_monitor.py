# monitor.py

import os
import re
import json
import logging
import hashlib               # â† Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ¸Ğ»Ğ¸
import requests

from datetime import datetime
from urllib.parse import urljoin
from bs4 import BeautifulSoup
from telegram import Update
from telegram.ext import (
    ApplicationBuilder,
    CommandHandler,
    ContextTypes,
)

import config

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
logging.basicConfig(
    format="%(asctime)s [%(levelname)s] %(message)s",
    level=logging.INFO
)
logger = logging.getLogger(__name__)

session = requests.Session()
session.headers["User-Agent"] = "Mozilla/5.0"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def load_state() -> dict:
    """Ğ“Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼ ĞºĞ»ÑÑ‡Ğ¸ last_pdf, last_pdf_hash Ğ¸ last_news_url."""
    if os.path.exists(config.STATE_FILE):
        st = json.load(open(config.STATE_FILE, "r", encoding="utf-8"))
    else:
        st = {}
    st.setdefault("last_pdf", None)
    st.setdefault("last_pdf_hash", None)    # â† Ğ½Ğ¾Ğ²Ñ‹Ğ¹ ĞºĞ»ÑÑ‡
    st.setdefault("last_news_url", None)
    return st

def save_state(st: dict):
    with open(config.STATE_FILE, "w", encoding="utf-8") as f:
        json.dump(st, f, ensure_ascii=False, indent=2)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def fetch_latest_pdf() -> tuple[str, str] | tuple[None, None]:
    resp = session.get(config.PAGE_URL)
    resp.raise_for_status()
    soup = BeautifulSoup(resp.text, "html.parser")

    candidates = []
    for a in soup.find_all("a", href=True):
        m = re.search(r"(free_flats_(\d{8})\.pdf)$", a["href"])
        if not m:
            continue
        fn   = m.group(1)
        ds   = m.group(2)
        dt = None
        for fmt in ("%Y%m%d","%d%m%Y"):
            try:
                dt = datetime.strptime(ds, fmt)
                break
            except ValueError:
                continue
        if not dt:
            continue
        url = urljoin(config.BASE_URL, a["href"])
        candidates.append((dt, fn, url))

    if not candidates:
        return None, None
    _, fname, furl = max(candidates, key=lambda x: x[0])
    return fname, furl

def fetch_latest_news() -> tuple[str, str] | tuple[None, None]:
    resp = session.get(config.NEWS_PAGE_URL)
    resp.raise_for_status()
    soup = BeautifulSoup(resp.text, "html.parser")

    a = soup.find("a", href=re.compile(r"^/novosti/"))
    if not a:
        return None, None

    title = a.get_text(strip=True)
    url   = urljoin(config.BASE_URL, a["href"])
    return title, url

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async def scheduled_pdf(context: ContextTypes.DEFAULT_TYPE):
    st           = load_state()
    last_hash    = st["last_pdf_hash"]       # â† Ğ±ĞµÑ€Ñ‘Ğ¼ ÑÑ‚Ğ°Ñ€Ñ‹Ğ¹ Ñ…ĞµÑˆ
    fname, furl  = fetch_latest_pdf()
    if not fname:
        return

    # ÑĞºĞ°Ñ‡Ğ¸Ğ²Ğ°ĞµĞ¼ Ğ¸ ÑÑ‡Ğ¸Ñ‚Ğ°ĞµĞ¼ Ñ…ĞµÑˆ
    logger.info(f"Downloading PDF for hash check: {furl}")
    r = session.get(furl)
    r.raise_for_status()
    data = r.content
    new_hash = hashlib.sha256(data).hexdigest()

    # ĞµÑĞ»Ğ¸ Ñ…ĞµÑˆ Ğ½Ğµ Ğ¸Ğ·Ğ¼ĞµĞ½Ğ¸Ğ»ÑÑ â€” Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ¸Ğ¼
    if new_hash == last_hash:
        logger.info("PDF hash unchanged, skipping")
        return

    # ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ñ„Ğ°Ğ¹Ğ» Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾
    local = os.path.join("downloads", fname)
    os.makedirs(os.path.dirname(local), exist_ok=True)
    with open(local, "wb") as f:
        f.write(data)

    # ÑˆĞ»Ñ‘Ğ¼ Ğ² Ñ‚ĞµĞ»ĞµĞ³Ñ€Ğ°Ğ¼
    await context.bot.send_message(
        chat_id=config.CHAT_ID,
        text="âœ… Ğ’Ñ‹ÑˆĞ»Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ñ€ĞµĞ´Ğ°ĞºÑ†Ğ¸Ñ Ñ„Ğ°Ğ¹Ğ»Ğ°"
    )
    await context.bot.send_document(
        chat_id=config.CHAT_ID,
        document=open(local, "rb")
    )
    logger.info(f"Sent PDF {fname}")

    # Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑĞµĞ¼ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ
    st["last_pdf"]      = fname
    st["last_pdf_hash"] = new_hash    # â† ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ñ…ĞµÑˆ
    save_state(st)

async def scheduled_news(context: ContextTypes.DEFAULT_TYPE):
    st            = load_state()
    last_news_url = st["last_news_url"]

    title, url = fetch_latest_news()
    if not url or url == last_news_url:
        return

    text = f"ğŸ“° ĞĞ¾Ğ²Ğ°Ñ Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚ÑŒ:\n{title}\n{url}"
    await context.bot.send_message(chat_id=config.CHAT_ID, text=text)
    logger.info(f"Sent news {url}")

    st["last_news_url"] = url
    save_state(st)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async def cmd_state(update: Update, ctx: ContextTypes.DEFAULT_TYPE):
    st = load_state()
    await update.message.reply_text(
        f"Ğ¢ĞµĞºÑƒÑ‰ĞµĞµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ:\n```json\n{json.dumps(st, indent=2, ensure_ascii=False)}\n```",
        parse_mode="MarkdownV2"
    )

async def cmd_start(update: Update, ctx: ContextTypes.DEFAULT_TYPE):
    await update.message.reply_text(
        "ĞŸÑ€Ğ¸Ğ²ĞµÑ‚! Ğ¡Ğ»ĞµĞ¶Ñƒ Ğ·Ğ° PDF Ğ¸ Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚ÑĞ¼Ğ¸.\n"
        "ĞšĞ¾Ğ¼Ğ°Ğ½Ğ´Ñ‹:\n"
        "/getpdf â€” Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ğ¹ PDF\n"
        "/getnews â€” Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ñ‚ĞµĞºÑƒÑ‰ÑƒÑ Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚ÑŒ\n"
        "/state â€” Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ÑŒ ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ñ‘Ğ½Ğ½Ğ¾Ğµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ"
    )

async def cmd_getpdf(update: Update, ctx: ContextTypes.DEFAULT_TYPE):
    fname, furl = fetch_latest_pdf()
    if not fname:
        return await update.message.reply_text("PDF Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½.")
    # ÑĞºĞ°Ñ‡Ğ¸Ğ²Ğ°ĞµĞ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ Ğ´Ğ»Ñ Ñ€ÑƒÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°, Ğ±ĞµĞ· Ñ…ĞµÑˆĞµĞ¹
    local = os.path.join("downloads", fname)
    os.makedirs(os.path.dirname(local), exist_ok=True)
    r = session.get(furl, stream=True)
    r.raise_for_status()
    with open(local, "wb") as f:
        for chunk in r.iter_content(32_768):
            f.write(chunk)
    await ctx.bot.send_message(chat_id=update.effective_chat.id,
                               text="âœ… Ğ¢ĞµĞºÑƒÑ‰Ğ¸Ğ¹ PDF:")
    await ctx.bot.send_document(chat_id=update.effective_chat.id,
                                document=open(local, "rb"))

async def cmd_getnews(update: Update, ctx: ContextTypes.DEFAULT_TYPE):
    title, url = fetch_latest_news()
    if not url:
        return await update.message.reply_text("ĞĞ¾Ğ²Ğ¾ÑÑ‚ĞµĞ¹ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾.")
    await ctx.bot.send_message(
        chat_id=update.effective_chat.id,
        text=f"ğŸ“° Ğ¢ĞµĞºÑƒÑ‰Ğ°Ñ Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚ÑŒ:\n{title}\n{url}"
    )

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    app = (
        ApplicationBuilder()
        .token(config.BOT_TOKEN)
        .build()
    )

    jq = app.job_queue
    jq.run_repeating(scheduled_pdf,
                     interval=config.CHECK_EVERY_MINUTES * 60,
                     first=5)
    jq.run_repeating(scheduled_news,
                     interval=config.NEWS_CHECK_INTERVAL * 60,
                     first=10)

    app.add_handler(CommandHandler("start",  cmd_start))
    app.add_handler(CommandHandler("state",  cmd_state))
    app.add_handler(CommandHandler("getpdf", cmd_getpdf))
    app.add_handler(CommandHandler("getnews",cmd_getnews))

    logger.info("Bot started, pollingâ€¦")
    app.run_polling()

if __name__ == "__main__":
    main()
